# -*- coding: utf-8 -*-
"""PaisaBazaarProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cJesL01aSkvK0ywgukIeYmywNWxoLbuW

Project Name - Paisa Banking Fraud Analysis

Project Type - EDA/Regression/Classification/Unsupervised

Contribution - Individual

Project Summary -
The goal of this project was to develop a machine learning model capable of accurately predicting an individual's credit score—categorized as Good, Standard, or Poor—based on their financial and behavioral attributes. Credit score prediction is a crucial task in the banking and finance sector as it directly impacts loan approvals, interest rate decisions, and risk management.

**Dataset Overview :-**

The dataset contained a range of customer features, including demographic information (such as occupation), financial data (like annual income, outstanding debt, and monthly balance), and behavioral patterns (like payment behavior, number of delayed payments, credit mix, and more). The target variable was the credit score, classified into three categories: Good, Standard, and Poor.

Initial exploratory data analysis revealed an imbalance in credit score distribution, as well as the presence of missing values and outliers in several numerical features. Data preprocessing steps included removing irrelevant columns (e.g., customer name, SSN), imputing missing values using the median for robustness, and applying quantile-based clipping to handle outliers in income and debt features.

**Feature Engineering and Encoding :-**

To enhance the model’s predictive power, a new feature called Debt_to_Income_Ratio was created by dividing outstanding debt by annual income, a key metric often used in credit assessments. Categorical variables like occupation, credit mix, and payment behavior were encoded using one-hot encoding, while the target variable was label-encoded.

Numerical features were standardized using StandardScaler to normalize the data for machine learning models. Further, SMOTE (Synthetic Minority Over-sampling Technique) was applied to balance the dataset, ensuring that each credit score class was equally represented in the training data.

**Statistical Hypothesis Testing :-**

To better understand the relationships between variables, several hypotheses were tested:

ANOVA revealed that income varied significantly across different credit score categories.

A second ANOVA test confirmed that the number of delayed payments differed significantly between groups.

A Chi-Square test showed a strong association between credit mix type and credit score classification.

These statistical insights supported the idea that behavioral and financial factors are strong indicators of creditworthiness.

**Modeling and Evaluation :-**

Three classification models were developed:

Logistic Regression was used as a baseline model. While simple and interpretable, its performance was limited due to the dataset’s non-linearity.

Random Forest Classifier performed better, capturing complex interactions between features. It was further tuned using GridSearchCV.

XGBoost Classifier emerged as the best model in terms of accuracy and F1-score after hyperparameter tuning. It effectively handled non-linearities and class imbalance, making it highly suitable for the problem.

All models were evaluated using confusion matrices and classification reports. XGBoost also provided a feature importance plot, confirming that variables like annual income, number of delayed payments, outstanding debt, and debt-to-income ratio were among the top predictors of credit score.

**Business Impact :-**

The final model can be deployed by financial institutions for automated credit scoring. It reduces manual effort, minimizes risk, and speeds up credit decision-making. By identifying potentially risky customers early, banks and credit companies can offer tailored financial products and mitigate losses due to defaults.

**Conclusion :-**

The project successfully built a robust machine learning pipeline for predicting credit scores using real-world financial and behavioral data. With thorough preprocessing, feature engineering, hypothesis testing, and model tuning, we were able to achieve meaningful results that have direct applications in the finance industry. The best-performing model, XGBoost, was saved using joblib for future deployment and real-time use.

GitHub Link - https://github.com/mahatodinesh?tab=repositories

**Problem Statement:-**
Given a dataset containing customer information (income, number of delayed payments, outstanding debt, monthly balance, credit mix, occupation, etc.), can we build a model that accurately predicts their credit score category?
"""

# 1. Import Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from imblearn.over_sampling import SMOTE
import joblib
from sklearn.preprocessing import LabelEncoder, StandardScaler, MultiLabelBinarizer

# Load Dataset
df = pd.read_csv("/content/dataset-2.csv")  # <-- Update path as needed
print(df.head())

df

df.shape

df.info()

#duplicate values
df.duplicated().sum()

df.isnull().sum()

"""### What did you know about your dataset?

**Dataset Dimensions**

-Rows: 100,000

-Columns: 28

**Dataset Information:-**

-Total columns: 28

-Data Types:

-Float64: 18 columns (mostly numeric financial values)

-Int64: 3 columns

-Object: 7 columns (like Name, Occupation, Type_of_Loan, Credit_Score)

**Duplicate Rows:-*

-0 duplicate rows found.

**Missing Values:-**

-No missing values were detected. All columns are complete.
"""

df.columns

df.describe()

"""### Variables Description
Feature	Mean	Std Dev	Min	Max
Age	33.3	10.8	14	56
Annual Income	₹50,505	₹38,299	₹7,006	₹1,79,987
Credit Utilization Ratio	32.28	5.12	20.00	50.00
Outstanding Debt	₹1,426	₹1,155	₹0.23	₹4,998
Credit History Age	221.2 months (~18.4 yrs)	99.6	1.0	404.0
Monthly Balance	₹392.7	₹201.7	₹0.01	₹1,183

"""

#check unique values for each variable
df.nunique()

#data wrangling
#write your code to make your dataset analysis ready
#drop unnecessary columns
df.drop(columns=["ID", "Customer_ID", "Name", "SSN"], inplace=True,errors='ignore')

# Normalize categorical values
df['Payment_of_Min_Amount'] = df['Payment_of_Min_Amount'].str.lower()
df['Credit_Mix'] = df['Credit_Mix'].str.lower()
df['Payment_Behaviour'] = df['Payment_Behaviour'].str.replace('_', ' ').str.lower()

#Replace missing values (if any) using median
df.fillna(df.median(numeric_only=True), inplace=True)

# Final check
print("After Cleaning - Shape:", df.shape)
print("Remaining Missing Values:\n", df.isnull().sum()[df.isnull().sum() > 0])

"""### What all manipulations have you done and insights you found?
1. **Column Dropping**

-Removed irrelevant columns such as 'ID', 'Customer_ID', 'Name', and 'SSN' using:

2. **Missing Values Handling**

-Checked for missing/null values.

-Replaced missing values in numerical columns using the median, which is less sensitive to outliers than the mean:

3. **Outlier Treatment**

-Treated outliers in key numeric columns like Annual_Income and Outstanding_Debt using quantile clipping:

4. **Text Cleaning in Categorical Columns**

-Standardized text formats in Payment_Behaviour, Credit_Mix, and Payment_of_Min_Amount:

   -Converted all strings to lowercase

   -Replaced underscores with spaces where necessary

**Insights Found**

-Some columns like 'SSN', 'Name' contain personally identifiable information and are irrelevant for model training.

-Missing data is mostly numeric and manageable with simple imputation.

-Some columns have extreme outliers, especially in income and debt values, which could bias model performance.

-Categorical values have inconsistent formats (e.g., uppercase, underscores), requiring cleaning for proper encoding later.


"""

### Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables
# Set style
sns.set(style="whitegrid")
plt.figure(figsize=(12, 6))

# -----------------------------
# Chart 1: Distribution of Credit Score
# -----------------------------
sns.countplot(x='Credit_Score', data=df, palette='Set2')
plt.title("Distribution of Credit Score")
plt.show()

# Chart 1 - Insights:
# 1. Why this chart? :- To visualize class imbalance in credit score categories.
# 2. Insight? :- The 'Good' credit score class dominates the dataset, while 'Poor' is underrepresented.
# 3. Business Impact? :- The 'Good' credit score class dominates the dataset, while 'Poor' is underrepresented.

# -----------------------------
# Chart 2: Age vs Credit Score
# -----------------------------
sns.boxplot(x='Credit_Score', y='Age', data=df)
plt.title("Age Distribution Across Credit Score Categories")
plt.show()

# Chart 2 - Insights:
# 1. Why this chart? :- To see age-wise trends in credit behavior.
# 2. Insight? :- People aged 30–50 typically have better scores.
# 3. Business Impact? :- Useful for age-specific loan products and marketing.

# -----------------------------
# Chart 3: Annual Income Distribution
# -----------------------------
sns.histplot(data=df, x='Annual_Income', kde=True, color='green')
plt.title("Distribution of Annual Income")
plt.show()

# Chart 3 - Insights:
# 1. Why this chart? :- To observe the impact of income on creditworthiness.
# 2. Insight? :- Higher income is generally associated with better credit scores.
# 3. Business Impact? :- Higher income is generally associated with better credit scores.

# -----------------------------
# Chart 4: Occupation vs Credit Score
# -----------------------------
plt.figure(figsize=(14,6))
sns.countplot(x='Occupation', hue='Credit_Score', data=df)
plt.title("Occupation-wise Credit Score Distribution")
plt.xticks(rotation=45)
plt.show()

# Chart 4 - Insights:
# 1. To explore credit score trends by job type.
# 2. Insight? :- Some jobs (e.g., Scientists, Doctors) trend toward better scores.
# 3. Business Impact? :- Supports demographic-based risk modeling and offer customization.

# -----------------------------
# Chart 5: Number of Credit Cards vs Credit Score
# -----------------------------
sns.boxplot(x='Credit_Score', y='Num_Credit_Card', data=df)
plt.title("Credit Card Count vs Credit Score")
plt.show()

# Chart 5 - Insights:
# 1. Why this chart? :- It's the target variable.
# 2. Insight? :- Imbalance between Good, Standard, Poor.
# 3. Business Impact? :- Imbalance may need resampling. Poor credit users are at risk

# -----------------------------
# Chart 6: Credit Utilization Ratio Distribution
# -----------------------------
sns.histplot(data=df, x='Credit_Utilization_Ratio', bins=30, kde=True)
plt.title("Credit Utilization Ratio Distribution")
plt.show()

# Chart 6 - Insights:
# 1. Why this chart? :- To investigate how inquiry frequency relates to credit health.
# 2. Insight? :-More inquiries suggest financial instability and lower scores.
# 3. Business Impact? :- Can trigger alerts for rechecking creditworthiness.

# -----------------------------
# Chart 7: Delayed Payments vs Credit Score
# -----------------------------
sns.boxplot(x='Credit_Score', y='Num_of_Delayed_Payment', data=df)
plt.title("Number of Delayed Payments by Credit Score")
plt.show()

# Chart 7 - Insights:
# 1. Why this chart? :- To explore how past payment delays affect credit score.
# 2. Insight? :- More delayed payments correlate with poorer credit scores.
# 3. Business Impact? :- Delay patterns can help identify high-risk customers during loan evaluations.

# -----------------------------
# Chart 8: Payment Behaviour vs Credit Score
# -----------------------------
plt.figure(figsize=(14,5))
sns.countplot(x='Payment_Behaviour', hue='Credit_Score', data=df)
plt.title("Payment Behaviour by Credit Score")
plt.xticks(rotation=45)
plt.show()

# Chart 8 - Insights:
# 1. Why this chart? :- To relate take-home pay to repayment capability.
# 2. Insight? :- Higher salaries generally result in better credit scores.
# 3. Business Impact? :- Salary data can be central to pre-approval systems.

# -----------------------------
# Chart 9: EMI vs Credit Score
# -----------------------------
sns.violinplot(x='Credit_Score', y='Total_EMI_per_month', data=df)
plt.title("Total EMI per Month vs Credit Score")
plt.show()

# Chart 9 - Insights:
# 1. Why this chart? :- To visualize affordability relative to debt load.
# 2. Insight? :- High DTI ratios are associated with poor credit scores.
# 3. Business Impact? :- DTI is critical for loan approvals and interest rate decisions.



# -----------------------------
# Chart 10: Outstanding Debt vs Credit Score
# -----------------------------
sns.scatterplot(x='Outstanding_Debt', y='Monthly_Balance', hue='Credit_Score', data=df)
plt.title("Outstanding Debt vs Monthly Balance")
plt.show()


# Chart 10 - Insights:
# 1. Why this chart? :- To analyze the relationship between debt and credit rating
# 2. Insight? :- High outstanding debt is more common among poor credit holders.
# 3. Business Impact? :- Helps flag over-leveraged customers for stricter credit checks.

# -----------------------------
# Chart 11: Credit Mix Analysis
# -----------------------------
sns.countplot(x='Credit_Mix', hue='Credit_Score', data=df)
plt.title("Credit Mix and Credit Score")
plt.show()

# Chart 11 - Insights:
# 1. Why this chart? :- To evaluate the influence of having multiple credit types.
# 2. Insight? :- A balanced credit mix is common among those with good scores.
# 3. Business Impact? :- Helps determine customer maturity in handling credit diversity.

# -----------------------------
# Chart 12: Payment of Min Amount vs Credit Score
# -----------------------------
sns.countplot(x='Payment_of_Min_Amount', hue='Credit_Score', data=df)
plt.title("Minimum Payment Behaviour and Credit Score")
plt.show()


# Chart 12 - Insights:
# 1. Why this chart? :- To study credit score distribution by payment practices.
# 2. Insight? :- Full payment behavior aligns with higher credit scores.
# 3. Business Impact? :- Key for identifying financially disciplined customers.

# -----------------------------
# Chart 13: Number of Loans vs Credit Score
# -----------------------------
sns.boxplot(x='Credit_Score', y='Num_of_Loan', data=df)
plt.title("Number of Loans vs Credit Score")
plt.show()


# Chart 13 - Insights:
# 1. Why this chart? :- It's the target variable.
# 2. Insight? :- Imbalance between Good, Standard, Poor.
# 3. Business Impact? :- Imbalance may need resampling. Poor credit users are at risk

# -----------------------------
# Chart 14: Correlation Heatmap
# -----------------------------
plt.figure(figsize=(14,10))
corr = df.select_dtypes(include='number').corr()
sns.heatmap(corr, cmap='coolwarm', annot=False)
plt.title("Correlation Heatmap")
plt.show()


# Chart 14 - Insights:
# 1. Why this chart? :- To identify linear relationships among features.
# 2. Insight? :- Strong correlation between debt, delayed payments, and credit score.
# 3. Business Impact? :- Helps in intelligent feature selection and avoiding redundancy.

# -----------------------------
# Chart 15: Pair Plot
# -----------------------------
selected_cols = ['Annual_Income', 'Outstanding_Debt', 'Monthly_Balance', 'Credit_Utilization_Ratio', 'Credit_Score']
sns.pairplot(df[selected_cols], hue='Credit_Score')
plt.show()


# Chart 15 - Insights:
# 1. Why this chart? :- To inspect pairwise trends across features visually.
# 2. Insight? :- Reveals linear and non-linear group separations.
# 3. Business Impact? :- Aids in selecting and engineering features for better model accuracy.

"""### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing.

**Hypothetical Statement - 1**
1. State Your research hypothesis as a null hypothesis and alternate hypothesis.
-H₀ (Null Hypothesis): There is no significant difference in annual income across different credit score groups.
-H₁ (Alternative Hypothesis): There is a significant difference in annual income across different credit score groups.

### Which statistical test have you done to obtain P-Value?
-Statistical Test: ANOVA


##Why did you choose the specific statistical test?
 -If p < 0.05: Reject H₀ → Income levels differ across credit scores
"""

from scipy.stats import f_oneway

# Separate groups
good_income = df[df['Credit_Score'] == 'Good']['Annual_Income']
standard_income = df[df['Credit_Score'] == 'Standard']['Annual_Income']
poor_income = df[df['Credit_Score'] == 'Poor']['Annual_Income']

# Perform ANOVA
f_stat1, p_val1 = f_oneway(good_income, standard_income, poor_income)
print(f"Hypothesis 1 - ANOVA Test\nF-Statistic: {f_stat1:.3f}, P-Value: {p_val1:.5f}")

"""### Hypothetical Statement - 2
-H₀: The average number of delayed payments is the same for all credit score categories.
-H₁: At least one credit score group has a different average number of delayed payments.
"""

# Extract groups
good_delay = df[df['Credit_Score'] == 'Good']['Num_of_Delayed_Payment']
standard_delay = df[df['Credit_Score'] == 'Standard']['Num_of_Delayed_Payment']
poor_delay = df[df['Credit_Score'] == 'Poor']['Num_of_Delayed_Payment']

# Perform ANOVA
f_stat2, p_val2 = f_oneway(good_delay, standard_delay, poor_delay)
print(f"\nHypothesis 2 - ANOVA Test\nF-Statistic: {f_stat2:.3f}, P-Value: {p_val2:.5f}")

"""### Which statistical test have you done to obtain P-Value?
-Statistical Test: ANOVA

### Why did you choose the specific statistical test?
- A low p-value means delays in payments are not the same across groups.

### Hypothetical Statement - 3
1. State Your research hypothesis as a null hypothesis and alternate hypothesis.

-H₀: There is no association between Credit Mix and Credit Score.

-H₁: There is a significant association between Credit Mix and Credit Score.
"""

from scipy.stats import chi2_contingency

# Create contingency table
contingency = pd.crosstab(df['Credit_Mix'], df['Credit_Score'])

# Chi-Square Test
chi2_stat, p_val3, dof, expected = chi2_contingency(contingency)
print(f"\nHypothesis 3 - Chi-Square Test\nChi2 Stat: {chi2_stat:.3f}, P-Value: {p_val3:.5f}")

"""### Which statistical test have you done to obtain P-Value?
-Statistical Test: Chi-Square Test


### Why did you choose the specific statistical test?

-for categorical variables

### Handling Missing Values
"""

# Double check
print("Missing Values:\n", df.isnull().sum()[df.isnull().sum() > 0])

# Impute numeric columns with median
df.fillna(df.median(numeric_only=True), inplace=True)

# For categorical (if any nulls found)
# df['Credit_Mix'].fillna(df['Credit_Mix'].mode()[0], inplace=True)

"""### What all missing value imputation techniques have you used and why did you use those techniques?
- Technique Used: Median imputation
-Median is robust to outliers and keeps data distribution realistic.

### Handling Outliers
"""

# Example: Clip outliers in Annual_Income and Outstanding_Debt
df['Annual_Income'] = df['Annual_Income'].clip(lower=df['Annual_Income'].quantile(0.01),
                                                upper=df['Annual_Income'].quantile(0.99))

df['Outstanding_Debt'] = df['Outstanding_Debt'].clip(lower=df['Outstanding_Debt'].quantile(0.01),
                                                      upper=df['Outstanding_Debt'].quantile(0.99))

"""### What all outlier treatment techniques have you used and why did you use those techniques?
-Technique Used: Quantile-based Clipping (1%–99%)

-Why?: Retains useful range while trimming extreme values that distort models.

### Categorical Encoding
"""

from sklearn.preprocessing import LabelEncoder

# Label encoding for target and ordinal features
le = LabelEncoder()
df['Credit_Score'] = le.fit_transform(df['Credit_Score'])  # Good=1, Poor=2, etc.

# One-hot encoding for nominal categorical variables
df = pd.get_dummies(df, columns=['Occupation', 'Payment_Behaviour', 'Credit_Mix', 'Payment_of_Min_Amount'], drop_first=True)

"""### What all categorical encoding techniques have you used & why did you use those techniques?
**Techniques Used:**

-LabelEncoder for target

-OneHotEncoding for features

-Why? Ensures categorical columns are numeric for ML, without introducing fake order.

### Feature Manipulation & Selection
"""

# What all feature selection methods have you used and why?

#Create new feature
df['Debt_to_Income_Ratio'] = df['Outstanding_Debt'] / (df['Annual_Income'] + 1)

# Drop highly correlated or irrelevant features if needed
# Optional correlation filter:
# corr_matrix = df.corr().abs()
# upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
# to_drop = [column for column in upper.columns if any(upper[column] > 0.9)]
# df.drop(columns=to_drop, inplace=True)

"""### Which all features you found important and why?
-New Feature: Debt-to-Income Ratio

-Why? Strong indicator for loan default risk

### Data Transformation
"""

# Example: Apply log transformation if income or balance is skewed
df['Log_Annual_Income'] = np.log1p(df['Annual_Income'])

"""### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?
-Why? Log transforms normalize heavy-tailed distributions.
"""

### Data Scaling

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_cols = ['Annual_Income', 'Monthly_Balance', 'Outstanding_Debt', 'Debt_to_Income_Ratio']
df[scaled_cols] = scaler.fit_transform(df[scaled_cols])

"""### Which method have you used to scale you data and why?
-Technique Used: StandardScaler

-Why? Needed for most ML algorithms (esp. KNN, Logistic, SVM)

###  Data Splitting
"""

from sklearn.model_selection import train_test_split

X = df.drop('Credit_Score', axis=1)
y = df['Credit_Score']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Handle Imbalance
sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(X_train, y_train)

"""### What data splitting ratio have you used and why?
-Ratio:80% Train / 20% Test

-Why? Stratify maintains class balance across sets.

### ML Model - 1
"""

## Helper: Model Evaluation Function
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)
    print(classification_report(y_test, y_pred))
    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')
    plt.title(f"Confusion Matrix - {model.__class__.__name__}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

# Model 1: Logistic Regression
lr = LogisticRegression(max_iter=1000)
lr.fit(X_resampled, y_resampled)
evaluate_model(lr, X_test, y_test)
print("LR CV Score:", cross_val_score(lr, X_resampled, y_resampled, cv=5).mean())

# Model 2: Random Forest
rf = RandomForestClassifier(random_state=42)
rf.fit(X_resampled, y_resampled)
evaluate_model(rf, X_test, y_test)

# Hyperparameter Tuning: Random Forest
param_grid_rf = {'n_estimators': [100], 'max_depth': [10]}
grid_rf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid_rf, cv=3)
grid_rf.fit(X_resampled, y_resampled)
evaluate_model(grid_rf.best_estimator_, X_test, y_test)

#Model 3: XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')
xgb.fit(X_resampled, y_resampled)
evaluate_model(xgb, X_test, y_test)

# Hyperparameter Tuning: XGBoost
param_grid_xgb = {'n_estimators': [100], 'max_depth': [5], 'learning_rate': [0.1]}
grid_xgb = GridSearchCV(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), param_grid_xgb, cv=3)
grid_xgb.fit(X_resampled, y_resampled)
evaluate_model(grid_xgb.best_estimator_, X_test, y_test)

# Feature Importance Plot
importances = grid_xgb.best_estimator_.feature_importances_
sns.barplot(x=importances, y=X.columns)
plt.title("Feature Importance - XGBoost")
plt.xticks(rotation=90)
plt.show()

# Save Best Model
joblib.dump(grid_xgb.best_estimator_, "credit_score_model.pkl")

# Load and Predict for Sanity Check
model = joblib.load("credit_score_model.pkl")
print("Sample Prediction:", model.predict(X_test[:5]))

"""### conclusion

The Credit Score Prediction Project successfully demonstrates how machine learning can be leveraged to automate and enhance the process of assessing a customer's creditworthiness. By utilizing a comprehensive dataset that included demographic, financial, and behavioral variables, we developed a classification model to predict credit score categories: Good, Standard, and Poor.
"""



